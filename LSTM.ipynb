{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42ddb554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb367dbb-2eaa-42c4-ad41-5170c399114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw_df = pd.read_csv('./features_raw.csv')\n",
    "emotions_df = pd.read_csv('./emotions.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5cea0f6-eda5-4a5c-9813-f0fcf2919afb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m features_raw_df\u001b[38;5;241m.\u001b[39mjoin(emotions_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Scale your features - important for neural network convergence\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "combined_df = features_raw_df.join(emotions_df[['label']])\n",
    "\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Scale your features - important for neural network convergence\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split your data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef5a04-e615-49d3-96b4-81517d00b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values in numeric columns\n",
    "numeric_cols = combined_df.select_dtypes(include=[np.number])\n",
    "for col in numeric_cols.columns:\n",
    "    # Fill columns with all NaN values with a placeholder (e.g., 0) or consider dropping them\n",
    "    if numeric_cols[col].isnull().all():\n",
    "        combined_df[col] = combined_df[col].fillna(0)\n",
    "    else:\n",
    "        # Fill remaining NaNs with the column mean\n",
    "        combined_df[col] = combined_df[col].fillna(numeric_cols[col].mean())\n",
    "\n",
    "# For non-numeric columns, decide on a strategy\n",
    "non_numeric_cols = combined_df.select_dtypes(exclude=[np.number])\n",
    "combined_df[non_numeric_cols.columns] = non_numeric_cols.fillna('unknown')\n",
    "\n",
    "# Check for any remaining NaN values and print the problematic columns\n",
    "nan_columns = combined_df.columns[combined_df.isnull().any()].tolist()\n",
    "if nan_columns:\n",
    "    print(f\"Columns with NaN: {nan_columns}\")\n",
    "    for col in nan_columns:\n",
    "        print(f\"NaN count in '{col}': {combined_df[col].isnull().sum()}\")\n",
    "else:\n",
    "    print(\"No NaN values remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06762d1-b077-45c6-922a-ec7d75a529e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    # RNN layer\n",
    "    # Note: 'input_shape' should match the shape of your features. \n",
    "    # For simplicity, we're assuming each feature is treated as a separate timestep.\n",
    "    # Adjust 'input_shape' as per your dataset's feature size.\n",
    "    SimpleRNN(50, input_shape=(X_train.shape[1], 1), activation='relu'),\n",
    "    \n",
    "    # Output layer\n",
    "    # The number of neurons in the last layer should match the number of classes.\n",
    "    # Use 'softmax' activation for multi-class classification.\n",
    "    Dense(y_encoded.max() + 1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7eb18-0376-425c-ae2f-a3cc3bae9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Inf values with NaN\n",
    "X_train[np.isinf(X_train)] = np.nan\n",
    "\n",
    "# Compute the mean of each column, ignoring NaNs\n",
    "col_mean = np.nanmean(X_train, axis=0)\n",
    "\n",
    "# Check if there are any NaN values in the column means (which would imply that those columns are all NaN)\n",
    "nan_cols = np.where(np.isnan(col_mean))\n",
    "\n",
    "# If there are, you might choose to set these all-NaN columns to zero or some other value that makes sense for your dataset\n",
    "X_train[:, nan_cols] = 0\n",
    "\n",
    "# Recalculate the mean of each column, ignoring NaNs, after replacing all-NaN columns\n",
    "col_mean = np.nanmean(X_train, axis=0)\n",
    "\n",
    "# Find indices of NaNs in X_train and replace them with the mean of their column\n",
    "inds = np.where(np.isnan(X_train))\n",
    "X_train[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "# Verify there are no more NaNs or Infs\n",
    "assert not np.isnan(X_train).any(), \"NaNs remain in X_train after cleaning.\"\n",
    "assert not np.isinf(X_train).any(), \"Infs remain in X_train after cleaning.\"\n",
    "\n",
    "# Now that the data is clean, reshape it for the RNN\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f792bdb-b9ed-458a-a42c-c6f8bbddf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Model configuration\n",
    "n_units = 50\n",
    "dropout_rate = 0.2\n",
    "optimizer_learning_rate = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "n_features = X_train_reshaped.shape[2]  # Assuming X_train_reshaped has the shape (n_samples, n_timesteps, n_features)\n",
    "n_classes = y_encoded.max() + 1  # Assuming y_encoded contains encoded class labels starting from 0\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "# Adding an LSTM layer with dropout; return_sequences=True is needed if you stack LSTM layers\n",
    "model.add(LSTM(units=n_units, input_shape=(None, n_features), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "# If you want to add another LSTM layer, make sure return_sequences is True in the first LSTM layer\n",
    "model.add(LSTM(units=n_units, activation='tanh'))\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "# Output layer with softmax activation for classification\n",
    "model.add(Dense(units=n_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with the Adam optimizer and learning rate with gradient clipping\n",
    "optimizer = Adam(learning_rate=optimizer_learning_rate, clipvalue=1.0)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_reshaped, \n",
    "    y_train, \n",
    "    batch_size=batch_size, \n",
    "    epochs=n_epochs, \n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d80e7-64af-4e2d-a924-96c2b7a522f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
